name: Fetch Polymarket Data and Deploy

on:
  schedule:
    # Run every 15 minutes
    - cron: '*/15 * * * *'
  workflow_dispatch: # Allow manual triggers
  push:
    branches: [ master ]

# Required for GitHub Pages deployment
permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  fetch-data:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true

    - name: Install dependencies
      run: uv sync --frozen

    - name: Restore previous data
      continue-on-error: true
      run: |
        # Download data/markets.json and history snapshots from GitHub Pages
        # This allows script to reuse cached statements and calculate price changes
        mkdir -p data/history
        BASE_URL="https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}"

        echo "â³ Downloading previous data from GitHub Pages..."

        # Download main markets.json for statement caching
        if curl -fsSL "$BASE_URL/data/markets.json" -o data/markets.json; then
          MARKET_COUNT=$(grep -c '"question"' data/markets.json || echo "unknown")
          echo "âœ… Restored markets.json ($MARKET_COUNT markets)"
        else
          echo "âš ï¸  No previous markets.json found"
        fi

        # Download history snapshots for price changes (1h, 24h, 7d ago)
        # Script needs 3 snapshots within: 55-65min, 23-25h, 6.5-7.5d ago
        echo ""
        echo "â³ Downloading history snapshots for price changes..."

        # Try to download snapshots at target times (workflows run every 15min)
        for TARGET in "1hour:60" "24hours:1440" "7days:10080"; do
          LABEL=${TARGET%:*}
          MINS=${TARGET#*:}
          FOUND_SNAPSHOT=false

          # Try Â±45min window around target (every 15min)
          for OFFSET in 0 -15 15 -30 30 -45 45; do
            TOTAL_MINS=$((MINS + OFFSET))
            TIMESTAMP=$(date -u -d "$TOTAL_MINS minutes ago" +%Y-%m-%d_%H-%M 2>/dev/null || date -u -v-${TOTAL_MINS}M +%Y-%m-%d_%H-%M)
            FILE="$TIMESTAMP.json"

            if curl -fsSL "$BASE_URL/data/history/$FILE" -o "data/history/$FILE" 2>/dev/null; then
              echo "  âœ… $LABEL: $FILE"
              FOUND_SNAPSHOT=true
              break
            fi
          done

          if [ "$FOUND_SNAPSHOT" = false ]; then
            echo "  âŒ $LABEL: No snapshot found in Â±45min window"
          fi
        done

        FOUND=$(ls -1 data/history/*.json 2>/dev/null | wc -l)
        echo ""
        if [ "$FOUND" -eq 3 ]; then
          echo "âœ… Downloaded 3/3 snapshots - price changes will calculate correctly"
        else
          echo "âš ï¸  Downloaded $FOUND/3 snapshots - some price changes will be null"
        fi

    - name: Fetch market data
      env:
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: uv run python scripts/fetch_markets.py

    - name: Prepare deployment files
      run: |
        mkdir -p _site

        # Debug: Show what's in data/ before copying
        echo "ðŸ“¦ Files in data/ before copying:"
        ls -lah data/ || echo "  (data/ doesn't exist)"
        echo ""
        echo "ðŸ“¦ Files in data/history/ before copying:"
        ls -lah data/history/ || echo "  (data/history/ doesn't exist)"
        echo ""

        cp -r *.html *.css *.js data/ _site/ 2>/dev/null || true
        # Copy any other web assets if they exist
        [ -d assets ] && cp -r assets _site/ || true
        [ -d images ] && cp -r images _site/ || true
        # Create empty .gitignore in _site to prevent root .gitignore from blocking data/
        echo "# Allow all files in deployment artifact" > _site/.gitignore

        # Debug: Show what ended up in _site/
        echo "ðŸ“¦ Files in _site/data/:"
        ls -lah _site/data/ || echo "  (_site/data/ doesn't exist)"
        echo ""
        echo "ðŸ“¦ Files in _site/data/history/:"
        ls -lah _site/data/history/ || echo "  (_site/data/history/ doesn't exist)"
        echo ""
        echo "ðŸ“Š Total files being uploaded:"
        find _site -type f | wc -l
        echo "ðŸ“Š Artifact size:"
        du -sh _site

    - name: Upload GitHub Pages artifact
      run: |
        # Temporarily rename .gitignore so upload-artifact doesn't exclude data/
        mv .gitignore .gitignore.bak
        echo "Disabled .gitignore for artifact upload"
      shell: bash

    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: _site

    - name: Restore .gitignore
      if: always()
      run: mv .gitignore.bak .gitignore
      shell: bash

  deploy:
    needs: fetch-data
    runs-on: ubuntu-latest

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
